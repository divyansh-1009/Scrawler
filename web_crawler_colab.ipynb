{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🤖 Agentic AI Web Crawler - Google Colab Edition\n",
        "\n",
        "**Schema-less AI-Powered Intelligent Web Crawling**\n",
        "\n",
        "This notebook uses:\n",
        "- **Ollama** with **deepseek-r1:14b** for AI-powered decisions\n",
        "- **Crawl4AI** for intelligent web crawling\n",
        "- **AI Navigation** - LLM figures out which links are relevant\n",
        "- **Schema-less Extraction** - LLM determines what's important to scrape\n",
        "- **Two-phase crawling** - Reconnaissance + Targeted Deep Dive\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📦 Step 1: Install Dependencies\n",
        "\n",
        "Install all required packages for web crawling and AI processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q crawl4ai>=0.3.0 ollama>=0.1.0 aiohttp>=3.9.0 beautifulsoup4>=4.12.0 lxml>=4.9.0\n",
        "\n",
        "print(\"✅ All dependencies installed successfully!\")\n",
        "\n",
        "# Install Playwright browsers (required by Crawl4AI)\n",
        "print(\"\\n📥 Installing Playwright browsers...\")\n",
        "!playwright install chromium\n",
        "!playwright install-deps chromium\n",
        "\n",
        "print(\"✅ Playwright browsers installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Step 2: Setup Ollama Connection\n",
        "\n",
        "**Important**: This notebook expects Ollama to be running locally or accessible via network.\n",
        "\n",
        "For Google Colab, you have two options:\n",
        "1. **Tunnel from local machine**: Run Ollama locally and use ngrok/cloudflare tunnel\n",
        "2. **Install Ollama in Colab**: Install and run Ollama directly in this notebook\n",
        "\n",
        "We'll use option 2 for simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama in Colab\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"✅ Ollama installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Ollama server in background\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Ollama server\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(5)  # Wait for server to start\n",
        "\n",
        "print(\"✅ Ollama server started!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull the deepseek-r1:14b model\n",
        "print(\"📥 Downloading deepseek-r1:14b model (this may take several minutes)...\")\n",
        "!ollama pull deepseek-r1:14b\n",
        "\n",
        "print(\"✅ Model downloaded and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Step 3: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from collections import defaultdict\n",
        "import ollama\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import NoExtractionStrategy\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🕷️ Step 4: Define the Agentic Web Crawler Class\n",
        "\n",
        "This class implements:\n",
        "- AI-powered objective analysis\n",
        "- Schema-less content extraction\n",
        "- Intelligent link navigation\n",
        "- Two-phase crawling strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImprovedAgenticWebCrawler:\n",
        "    \"\"\"An intelligent web crawler that uses AI to make navigation decisions and extract content schema-lessly.\"\"\"\n",
        "    \n",
        "    def __init__(self, decision_model: str = \"deepseek-r1:14b\", extraction_model: str = \"deepseek-r1:14b\", max_pages: int = 50):\n",
        "        self.decision_model = decision_model\n",
        "        self.extraction_model = extraction_model\n",
        "        self.max_pages = max_pages\n",
        "        self.visited_urls = set()\n",
        "        self.scraped_data = []\n",
        "        self.base_domain = None\n",
        "        self.crawl_objective = \"\"\n",
        "        self.crawl_objective_analysis = {}\n",
        "        self.desired_data_types = []\n",
        "        self.site_understanding = {\"site_type\": None, \"main_sections\": [], \"content_patterns\": defaultdict(list), \"high_value_url_patterns\": [], \"recommended_focus\": \"\"}\n",
        "        self.page_relevance_scores = {}\n",
        "        self.high_value_pages = []\n",
        "        self.current_phase = \"initialization\"\n",
        "        \n",
        "    async def analyze_user_objective(self, objective: str) -> Dict[str, Any]:\n",
        "        print(\"\\n🤖 Analyzing your objective with AI...\")\n",
        "        prompt = f\"\"\"You are helping to plan a web crawling operation. Analyze the user's objective and provide a detailed crawl strategy.\n",
        "\n",
        "USER'S OBJECTIVE: \"{objective}\"\n",
        "\n",
        "Analyze this objective and provide:\n",
        "1. What TYPE of data they're looking for (e.g., products, articles, contact info, documentation, etc.)\n",
        "2. What specific FIELDS or attributes they likely want extracted\n",
        "3. What sections of a website would be most valuable\n",
        "4. What URL patterns to prioritize\n",
        "5. What URL patterns to avoid\n",
        "\n",
        "Respond in JSON format:\n",
        "{{\n",
        "  \"data_types\": [\"primary type\", \"secondary type\"],\n",
        "  \"key_fields\": [\"field1\", \"field2\", \"field3\"],\n",
        "  \"valuable_sections\": [\"section1\", \"section2\"],\n",
        "  \"url_patterns_to_seek\": [\"pattern1\", \"pattern2\"],\n",
        "  \"url_patterns_to_avoid\": [\"pattern1\", \"pattern2\"],\n",
        "  \"extraction_strategy\": \"Description of how to approach extraction\",\n",
        "  \"success_criteria\": \"How to know when we have enough data\"\n",
        "}}\n",
        "\n",
        "Be specific and actionable.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = ollama.generate(model=self.decision_model, prompt=prompt)\n",
        "            response_text = response['response'].strip()\n",
        "            if '```json' in response_text:\n",
        "                json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            elif '```' in response_text:\n",
        "                json_match = re.search(r'```\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            analysis = json.loads(response_text)\n",
        "            print(\"\\n✓ Objective Analysis Complete:\")\n",
        "            print(f\"  • Data Types: {', '.join(analysis.get('data_types', []))}\")\n",
        "            print(f\"  • Key Fields: {', '.join(analysis.get('key_fields', []))}\")\n",
        "            print(f\"  • Focus Areas: {', '.join(analysis.get('valuable_sections', []))}\")\n",
        "            self.crawl_objective_analysis = analysis\n",
        "            self.desired_data_types = analysis.get('data_types', [])\n",
        "            return analysis\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Analysis error: {e}\")\n",
        "            return {\"data_types\": [\"general content\"], \"key_fields\": [\"title\", \"content\", \"links\"], \"valuable_sections\": [\"main content\"], \"url_patterns_to_seek\": [], \"url_patterns_to_avoid\": [\"login\", \"signup\", \"cart\"], \"extraction_strategy\": \"Extract all available content\", \"success_criteria\": \"Crawl specified number of pages\"}\n",
        "        \n",
        "    def _is_same_domain(self, url: str) -> bool:\n",
        "        if not self.base_domain:\n",
        "            return False\n",
        "        parsed = urlparse(url)\n",
        "        return parsed.netloc == self.base_domain\n",
        "    \n",
        "    def _extract_url_pattern(self, url: str) -> str:\n",
        "        parsed = urlparse(url)\n",
        "        path_parts = [p for p in parsed.path.split('/') if p]\n",
        "        pattern_parts = []\n",
        "        for part in path_parts:\n",
        "            if part.isdigit() or len(part) > 30:\n",
        "                pattern_parts.append('*')\n",
        "            else:\n",
        "                pattern_parts.append(part)\n",
        "        return '/' + '/'.join(pattern_parts) if pattern_parts else '/'\n",
        "    \n",
        "    def _find_similar_visited_urls(self, url: str, limit: int = 3) -> List[str]:\n",
        "        pattern = self._extract_url_pattern(url)\n",
        "        similar = []\n",
        "        for visited in self.visited_urls:\n",
        "            if self._extract_url_pattern(visited) == pattern:\n",
        "                similar.append(visited)\n",
        "                if len(similar) >= limit:\n",
        "                    break\n",
        "        return similar\n",
        "    \n",
        "    async def _extract_content_with_ai(self, html: str, url: str, markdown: str = \"\") -> Dict[str, Any]:\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        for tag in soup.find_all(['nav', 'header', 'footer', 'script', 'style']):\n",
        "            tag.decompose()\n",
        "        page_text = soup.get_text(separator='\\n', strip=True)\n",
        "        content_to_analyze = markdown[:4000] if markdown else page_text[:4000]\n",
        "        headers = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2', 'h3']) if h.get_text(strip=True)][:15]\n",
        "        main_links = []\n",
        "        for a in soup.find_all('a', href=True)[:30]:\n",
        "            text = a.get_text(strip=True)\n",
        "            if text and len(text) > 2:\n",
        "                main_links.append(text)\n",
        "        \n",
        "        prompt = f\"\"\"You are an expert content analyst evaluating a web page for relevance and extracting information.\n",
        "\n",
        "USER'S OBJECTIVE: {self.crawl_objective}\n",
        "\n",
        "TARGET DATA TYPES: {', '.join(self.desired_data_types)}\n",
        "KEY FIELDS TO EXTRACT: {', '.join(self.crawl_objective_analysis.get('key_fields', []))}\n",
        "\n",
        "PAGE URL: {url}\n",
        "\n",
        "PAGE HEADERS:\n",
        "{chr(10).join(headers[:10])}\n",
        "\n",
        "PAGE CONTENT (excerpt):\n",
        "{content_to_analyze}\n",
        "\n",
        "MAIN LINKS:\n",
        "{', '.join(main_links[:15])}\n",
        "\n",
        "YOUR TASK - ANALYZE CAREFULLY:\n",
        "\n",
        "1. **Understand the Objective**: Read the user's objective carefully and understand what they're really asking for\n",
        "\n",
        "2. **Evaluate Page Relevance** (0-10 scale):\n",
        "   - 9-10: Directly answers the objective with specific, detailed information\n",
        "   - 7-8: Contains significant relevant information, clearly related to objective\n",
        "   - 5-6: Moderately relevant, has some useful information related to objective\n",
        "   - 3-4: Tangentially related, minor relevance or background information\n",
        "   - 1-2: Barely related, mostly irrelevant with tiny connection\n",
        "   - 0: Completely irrelevant (navigation, footer, unrelated content)\n",
        "\n",
        "3. **Extract Strategically**:\n",
        "   - For HIGH relevance (7+): Extract EVERYTHING in detail - full text, all items, complete data\n",
        "   - For MODERATE relevance (4-6): Extract key points and important details\n",
        "   - For LOW relevance (1-3): Extract only the specifically relevant parts\n",
        "   - Be thorough but focused on what actually relates to the objective\n",
        "\n",
        "4. **Quality Over Quantity**: \n",
        "   - Don't inflate relevance scores - be honest and accurate\n",
        "   - Extract complete information, but stay focused on the objective\n",
        "   - If a page is mostly navigation/footer/ads, score it low\n",
        "\n",
        "Respond in JSON format:\n",
        "{{\n",
        "  \"page_type\": \"...\",\n",
        "  \"relevance_score\": 0-10,\n",
        "  \"key_content\": {{\n",
        "    // Extract based on relevance level\n",
        "    // High relevance: comprehensive extraction\n",
        "    // Moderate: key points and details\n",
        "    // Low: only specifically relevant parts\n",
        "  }},\n",
        "  \"reasoning\": \"Clear explanation of WHY this page got this score and how it relates to the objective\",\n",
        "  \"content_summary\": \"Accurate summary of what useful information is on this page\"\n",
        "}}\n",
        "\n",
        "CRITICAL: Be accurate with relevance scores. Don't give high scores to barely related content.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = ollama.generate(model=self.extraction_model, prompt=prompt)\n",
        "            response_text = response['response'].strip()\n",
        "            if '```json' in response_text:\n",
        "                json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            elif '```' in response_text:\n",
        "                json_match = re.search(r'```\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            extracted = json.loads(response_text)\n",
        "            self._update_site_knowledge(url, extracted)\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ AI extraction error: {str(e)[:100]}\")\n",
        "            return {\"page_type\": \"unknown\", \"relevance_score\": 5, \"key_content\": {\"title\": soup.find('title').get_text() if soup.find('title') else \"No title\", \"headers\": headers, \"text_excerpt\": page_text[:500]}, \"reasoning\": \"Fallback extraction due to error\", \"content_summary\": \"Content extracted with fallback method\"}\n",
        "    \n",
        "    def _update_site_knowledge(self, url: str, extraction_result: Dict):\n",
        "        relevance = extraction_result.get('relevance_score', 0)\n",
        "        self.page_relevance_scores[url] = relevance\n",
        "        if relevance >= 6:  # Pages with moderate to high relevance\n",
        "            self.high_value_pages.append(url)\n",
        "            url_pattern = self._extract_url_pattern(url)\n",
        "            if url_pattern not in self.site_understanding['high_value_url_patterns']:\n",
        "                self.site_understanding['high_value_url_patterns'].append(url_pattern)\n",
        "            page_type = extraction_result.get('page_type')\n",
        "            if page_type:\n",
        "                self.site_understanding['content_patterns'][page_type].append({'url': url, 'pattern': url_pattern, 'relevance': relevance})\n",
        "    \n",
        "    def _normalize_url(self, url: str, base_url: str) -> str:\n",
        "        url = urljoin(base_url, url)\n",
        "        parsed = urlparse(url)\n",
        "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
        "    \n",
        "    async def _extract_links_with_context(self, html: str, base_url: str) -> List[Dict]:\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        links_with_context = []\n",
        "        avoid_patterns = self.crawl_objective_analysis.get('url_patterns_to_avoid', [])\n",
        "        avoid_patterns.extend(['javascript:', 'mailto:', 'tel:', '#', '.jpg', '.png', '.pdf', '.css', '.js'])\n",
        "        \n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link.get('href')\n",
        "            if any(pattern.lower() in href.lower() for pattern in avoid_patterns):\n",
        "                continue\n",
        "            try:\n",
        "                full_url = self._normalize_url(href, base_url)\n",
        "            except:\n",
        "                continue\n",
        "            if not self._is_same_domain(full_url) or full_url in self.visited_urls:\n",
        "                continue\n",
        "            anchor_text = link.get_text(strip=True)\n",
        "            if not anchor_text or len(anchor_text) < 2:\n",
        "                continue\n",
        "            parent = link.parent\n",
        "            context_text = parent.get_text(strip=True)[:200] if parent else \"\"\n",
        "            title = link.get('title', '')\n",
        "            aria_label = link.get('aria-label', '')\n",
        "            is_in_nav = bool(link.find_parent(['nav', 'header']))\n",
        "            is_in_main = bool(link.find_parent('main'))\n",
        "            is_prominent = bool(link.find_parent(['h1', 'h2', 'h3']))\n",
        "            \n",
        "            links_with_context.append({'url': full_url, 'anchor_text': anchor_text, 'context': context_text, 'title': title, 'aria_label': aria_label, 'is_navigation': is_in_nav, 'is_main_content': is_in_main, 'is_prominent': is_prominent, 'url_path': urlparse(full_url).path})\n",
        "        return links_with_context\n",
        "    \n",
        "    async def _score_url_relevance(self, link_info: Dict) -> Dict:\n",
        "        url = link_info['url']\n",
        "        similar_urls = self._find_similar_visited_urls(url)\n",
        "        historical_scores = [self.page_relevance_scores.get(u, 0) for u in similar_urls]\n",
        "        url_pattern = self._extract_url_pattern(url)\n",
        "        pattern_bonus = 2 if url_pattern in self.site_understanding['high_value_url_patterns'] else 0\n",
        "        heuristic_score = 5 + pattern_bonus\n",
        "        if link_info.get('is_main_content'):\n",
        "            heuristic_score += 1\n",
        "        if link_info.get('is_prominent'):\n",
        "            heuristic_score += 1\n",
        "        seek_patterns = self.crawl_objective_analysis.get('url_patterns_to_seek', [])\n",
        "        if any(pattern.lower() in url.lower() for pattern in seek_patterns):\n",
        "            heuristic_score += 2\n",
        "        return {'url': url, 'relevance_score': heuristic_score, 'historical_avg': sum(historical_scores) / len(historical_scores) if historical_scores else 5, 'should_crawl': heuristic_score >= 5, 'priority': 'high' if heuristic_score >= 8 else 'medium' if heuristic_score >= 6 else 'low'}\n",
        "    \n",
        "    async def _ask_ollama_for_navigation_advanced(self, current_url: str, available_links: List[Dict], page_extraction: Dict) -> List[str]:\n",
        "        if not available_links:\n",
        "            return []\n",
        "        scored_links = []\n",
        "        for link_info in available_links[:30]:\n",
        "            score_info = await self._score_url_relevance(link_info)\n",
        "            scored_links.append({**link_info, **score_info})\n",
        "        scored_links.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "        top_candidates = scored_links[:12]\n",
        "        links_summary = []\n",
        "        for i, link in enumerate(top_candidates, 1):\n",
        "            links_summary.append(f\"{i}. [{link['relevance_score']:.1f}] {link['anchor_text'][:50]} → {link['url_path']}\")\n",
        "        \n",
        "        prompt = f\"\"\"You are guiding a web crawler. Review these pre-scored URLs and select the best ones to crawl next.\n",
        "\n",
        "CRAWL OBJECTIVE: {self.crawl_objective}\n",
        "\n",
        "PROGRESS:\n",
        "- Pages crawled: {len(self.visited_urls)}/{self.max_pages}\n",
        "- High-value pages: {len(self.high_value_pages)}\n",
        "- Current phase: {self.current_phase}\n",
        "\n",
        "CURRENT PAGE: {current_url}\n",
        "Page type: {page_extraction.get('page_type', 'unknown')}\n",
        "Relevance: {page_extraction.get('relevance_score', '?')}/10\n",
        "Summary: {page_extraction.get('content_summary', 'N/A')}\n",
        "\n",
        "LEARNED PATTERNS:\n",
        "High-value URL patterns: {self.site_understanding['high_value_url_patterns'][:5]}\n",
        "\n",
        "TOP CANDIDATE URLS (with pre-scored relevance [score]):\n",
        "{chr(10).join(links_summary)}\n",
        "\n",
        "Select 3-5 URLs that best match the objective. Consider:\n",
        "1. URLs matching learned high-value patterns\n",
        "2. URLs with high relevance scores\n",
        "3. URLs that explore new areas vs going deeper\n",
        "4. Current progress toward objective\n",
        "\n",
        "Respond with ONLY the numbers (comma-separated, e.g., \"1,3,5,8\").\n",
        "If no links are worth crawling, respond \"NONE\".\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = ollama.generate(model=self.decision_model, prompt=prompt)\n",
        "            answer = response['response'].strip()\n",
        "            if 'NONE' in answer.upper():\n",
        "                print(\"  → AI: No valuable links found\")\n",
        "                return []\n",
        "            numbers = re.findall(r'\\d+', answer)\n",
        "            selected_urls = []\n",
        "            for num in numbers[:5]:\n",
        "                idx = int(num) - 1\n",
        "                if 0 <= idx < len(top_candidates):\n",
        "                    selected_urls.append(top_candidates[idx]['url'])\n",
        "            return selected_urls\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ Navigation AI error: {str(e)[:100]}\")\n",
        "            return [link['url'] for link in scored_links[:3]]\n",
        "    \n",
        "    async def _crawl_page(self, url: str, crawler: AsyncWebCrawler) -> Optional[Dict[str, Any]]:\n",
        "        try:\n",
        "            print(f\"📄 Crawling: {url}\")\n",
        "            result = await crawler.arun(url=url, extraction_strategy=NoExtractionStrategy(), bypass_cache=True)\n",
        "            if not result.success:\n",
        "                print(f\"  ✗ Failed to crawl\")\n",
        "                return None\n",
        "            ai_extraction = await self._extract_content_with_ai(result.html, url, result.markdown if result.markdown else \"\")\n",
        "            page_data = {\"url\": url, \"title\": result.metadata.get(\"title\", \"No title\"), \"description\": result.metadata.get(\"description\", \"\"), \"timestamp\": datetime.now().isoformat(), \"metadata\": result.metadata, \"ai_extraction\": ai_extraction, \"relevance_score\": ai_extraction.get('relevance_score', 0), \"page_type\": ai_extraction.get('page_type', 'unknown')}\n",
        "            relevance = ai_extraction.get('relevance_score', 0)\n",
        "            page_type = ai_extraction.get('page_type', 'unknown')\n",
        "            print(f\"  ✓ Type: {page_type} | Relevance: {relevance}/10\")\n",
        "            key_content = ai_extraction.get('key_content', {})\n",
        "            if key_content:\n",
        "                content_types = list(key_content.keys())\n",
        "                print(f\"  → Extracted: {', '.join(content_types[:3])}\")\n",
        "            return page_data\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error: {str(e)[:80]}\")\n",
        "            return None\n",
        "    \n",
        "    async def _analyze_site_structure(self) -> Dict:\n",
        "        print(\"\\n🔍 Analyzing site structure...\")\n",
        "        page_types = {}\n",
        "        for page in self.scraped_data:\n",
        "            page_type = page.get('page_type', 'unknown')\n",
        "            relevance = page.get('relevance_score', 0)\n",
        "            if page_type not in page_types:\n",
        "                page_types[page_type] = []\n",
        "            page_types[page_type].append(relevance)\n",
        "        pages_summary = []\n",
        "        for page in self.scraped_data[:10]:\n",
        "            pages_summary.append(f\"- {page['url']}: {page['page_type']} (relevance: {page.get('relevance_score', 0)}/10)\")\n",
        "        \n",
        "        prompt = f\"\"\"Analyze the reconnaissance crawl results and provide strategic guidance.\n",
        "\n",
        "CRAWL OBJECTIVE: {self.crawl_objective}\n",
        "\n",
        "PAGES CRAWLED IN RECONNAISSANCE ({len(self.scraped_data)} pages):\n",
        "{chr(10).join(pages_summary)}\n",
        "\n",
        "PAGE TYPE DISTRIBUTION:\n",
        "{json.dumps({pt: {'count': len(scores), 'avg_relevance': sum(scores)/len(scores)} for pt, scores in page_types.items()}, indent=2)}\n",
        "\n",
        "HIGH-VALUE URL PATTERNS DISCOVERED:\n",
        "{self.site_understanding['high_value_url_patterns']}\n",
        "\n",
        "Provide strategic analysis:\n",
        "1. What TYPE of website is this?\n",
        "2. Which sections/page types are most valuable for the objective?\n",
        "3. What URL patterns should we prioritize in deep crawl?\n",
        "4. What's the recommended crawl strategy going forward?\n",
        "\n",
        "Respond in JSON:\n",
        "{{\n",
        "  \"site_type\": \"...\",\n",
        "  \"most_valuable_page_types\": [\"type1\", \"type2\"],\n",
        "  \"recommended_focus\": \"Description of where to focus\",\n",
        "  \"high_priority_patterns\": [\"pattern1\", \"pattern2\"],\n",
        "  \"strategy\": \"continue_deep | adjust_objective | site_not_suitable\"\n",
        "}}\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = ollama.generate(model=self.decision_model, prompt=prompt)\n",
        "            response_text = response['response'].strip()\n",
        "            if '```json' in response_text:\n",
        "                json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            analysis = json.loads(response_text)\n",
        "            print(f\"  ✓ Site Type: {analysis.get('site_type', 'Unknown')}\")\n",
        "            print(f\"  ✓ Focus: {analysis.get('recommended_focus', 'General crawl')}\")\n",
        "            return analysis\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ Analysis error: {e}\")\n",
        "            return {\"site_type\": \"unknown\", \"most_valuable_page_types\": list(page_types.keys())[:2], \"recommended_focus\": \"Continue crawling all page types\", \"high_priority_patterns\": self.site_understanding['high_value_url_patterns'], \"strategy\": \"continue_deep\"}\n",
        "    \n",
        "    async def crawl_website(self, start_url: str) -> List[Dict[str, Any]]:\n",
        "        parsed_url = urlparse(start_url)\n",
        "        self.base_domain = parsed_url.netloc\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"🚀 STARTING INTELLIGENT WEB CRAWL\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Target: {start_url}\")\n",
        "        print(f\"Objective: {self.crawl_objective}\")\n",
        "        print(f\"Max pages: {self.max_pages}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        recon_budget = max(5, self.max_pages // 10)\n",
        "        self.current_phase = \"reconnaissance\"\n",
        "        print(f\"📡 PHASE 1: RECONNAISSANCE ({recon_budget} pages)\")\n",
        "        print(f\"{'─'*80}\")\n",
        "        \n",
        "        async with AsyncWebCrawler(verbose=False) as crawler:\n",
        "            url_queue = [start_url]\n",
        "            while url_queue and len(self.visited_urls) < recon_budget:\n",
        "                current_url = url_queue.pop(0)\n",
        "                if current_url in self.visited_urls:\n",
        "                    continue\n",
        "                self.visited_urls.add(current_url)\n",
        "                page_data = await self._crawl_page(current_url, crawler)\n",
        "                if page_data:\n",
        "                    self.scraped_data.append(page_data)\n",
        "                    result = await crawler.arun(url=current_url, bypass_cache=True)\n",
        "                    if result.success:\n",
        "                        links = await self._extract_links_with_context(result.html, current_url)\n",
        "                        if links:\n",
        "                            selected = links[:8]\n",
        "                            for link in selected:\n",
        "                                if link['url'] not in self.visited_urls and link['url'] not in url_queue:\n",
        "                                    url_queue.append(link['url'])\n",
        "                print(f\"  Progress: {len(self.visited_urls)}/{recon_budget} recon pages\\n\")\n",
        "        \n",
        "        site_analysis = await self._analyze_site_structure()\n",
        "        self.site_understanding.update(site_analysis)\n",
        "        \n",
        "        deep_budget = self.max_pages - len(self.visited_urls)\n",
        "        self.current_phase = \"deep_crawl\"\n",
        "        print(f\"\\n{'─'*80}\")\n",
        "        print(f\"🎯 PHASE 2: TARGETED DEEP CRAWL ({deep_budget} pages)\")\n",
        "        print(f\"{'─'*80}\\n\")\n",
        "        \n",
        "        async with AsyncWebCrawler(verbose=False) as crawler:\n",
        "            url_queue = []\n",
        "            for page in self.scraped_data:\n",
        "                if page.get('relevance_score', 0) >= 5:  # Include moderately relevant pages and above\n",
        "                    result = await crawler.arun(url=page['url'], bypass_cache=True)\n",
        "                    if result.success:\n",
        "                        links = await self._extract_links_with_context(result.html, page['url'])\n",
        "                        for link in links[:7]:  # Good balance of links to follow\n",
        "                            if link['url'] not in self.visited_urls:\n",
        "                                url_queue.append(link['url'])\n",
        "            url_queue = list(dict.fromkeys(url_queue))\n",
        "            \n",
        "            while url_queue and len(self.visited_urls) < self.max_pages:\n",
        "                current_url = url_queue.pop(0)\n",
        "                if current_url in self.visited_urls:\n",
        "                    continue\n",
        "                self.visited_urls.add(current_url)\n",
        "                page_data = await self._crawl_page(current_url, crawler)\n",
        "                if page_data:\n",
        "                    self.scraped_data.append(page_data)\n",
        "                    result = await crawler.arun(url=current_url, bypass_cache=True)\n",
        "                    if result.success:\n",
        "                        links = await self._extract_links_with_context(result.html, current_url)\n",
        "                        if links:\n",
        "                            selected_urls = await self._ask_ollama_for_navigation_advanced(current_url, links, page_data.get('ai_extraction', {}))\n",
        "                            print(f\"  → AI selected {len(selected_urls)} links\")\n",
        "                            for url in selected_urls:\n",
        "                                if url not in self.visited_urls and url not in url_queue:\n",
        "                                    url_queue.append(url)\n",
        "                print(f\"  Progress: {len(self.visited_urls)}/{self.max_pages} | Queue: {len(url_queue)} | High-value: {len(self.high_value_pages)}\\n\")\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"✅ CRAWL COMPLETE\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Total pages: {len(self.scraped_data)}\")\n",
        "        print(f\"High-value pages: {len(self.high_value_pages)}\")\n",
        "        print(f\"Avg relevance: {sum(self.page_relevance_scores.values())/len(self.page_relevance_scores) if self.page_relevance_scores else 0:.1f}/10\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        return self.scraped_data\n",
        "\n",
        "print(\"✅ ImprovedAgenticWebCrawler class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_content_recursive(content: Any, level: int = 3) -> list:\n",
        "    \"\"\"Recursively format extracted content into readable markdown.\"\"\"\n",
        "    lines = []\n",
        "    if isinstance(content, dict):\n",
        "        for key, value in content.items():\n",
        "            title = key.replace('_', ' ').title()\n",
        "            if isinstance(value, (dict, list)):\n",
        "                lines.append(f\"{'#' * level} {title}\\n\")\n",
        "                lines.extend(format_content_recursive(value, level + 1))\n",
        "            else:\n",
        "                lines.append(f\"**{title}**: {value}\\n\")\n",
        "    elif isinstance(content, list):\n",
        "        if not content:\n",
        "            lines.append(\"*No items found*\\n\")\n",
        "        elif all(isinstance(item, dict) for item in content):\n",
        "            for idx, item in enumerate(content, 1):\n",
        "                lines.append(f\"\\n### Item {idx}\\n\")\n",
        "                lines.extend(format_content_recursive(item, level + 1))\n",
        "        else:\n",
        "            for item in content:\n",
        "                if isinstance(item, str):\n",
        "                    lines.append(f\"- {item}\\n\")\n",
        "                else:\n",
        "                    lines.append(f\"- {str(item)}\\n\")\n",
        "    else:\n",
        "        lines.append(f\"{content}\\n\")\n",
        "    return lines\n",
        "\n",
        "def generate_human_readable_report(scraped_data: List[Dict], objective: str) -> str:\n",
        "    \"\"\"Generate a human-readable report from scraped data.\"\"\"\n",
        "    md = []\n",
        "    md.append(\"# 📊 Extracted Data Analysis Report\\n\")\n",
        "    md.append(f\"**Generated**: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\\n\")\n",
        "    md.append(f\"**Search Objective**: {objective}\\n\")\n",
        "    md.append(\"---\\n\")\n",
        "    relevant_pages = [p for p in scraped_data if p.get('relevance_score', 0) >= 5]\n",
        "    if not relevant_pages:\n",
        "        md.append(\"## No Relevant Data Found\\n\")\n",
        "        md.append(\"No pages matched the objective with sufficient relevance.\\n\")\n",
        "    else:\n",
        "        md.append(f\"## Summary\\n\")\n",
        "        md.append(f\"Found **{len(relevant_pages)}** pages with relevant information.\\n\\n\")\n",
        "        md.append(\"---\\n\")\n",
        "        for idx, page in enumerate(relevant_pages, 1):\n",
        "            url = page.get('url', 'Unknown URL')\n",
        "            ai_extraction = page.get('ai_extraction', {})\n",
        "            content = ai_extraction.get('key_content', {})\n",
        "            relevance = page.get('relevance_score', 0)\n",
        "            page_type = page.get('page_type', 'unknown')\n",
        "            summary = ai_extraction.get('content_summary', 'N/A')\n",
        "            if not content or content == {}:\n",
        "                continue\n",
        "            md.append(f\"\\n## 📄 Source {idx}\\n\")\n",
        "            md.append(f\"**Page Type**: {page_type}\\n\")\n",
        "            md.append(f\"**Relevance Score**: {relevance}/10\\n\")\n",
        "            md.append(f\"**Summary**: {summary}\\n\\n\")\n",
        "            md.extend(format_content_recursive(content, level=3))\n",
        "            md.append(f\"\\n**Source URL**: [{url}]({url})\\n\")\n",
        "            md.append(\"\\n---\\n\")\n",
        "    return ''.join(md)\n",
        "\n",
        "print(\"✅ Data formatting functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get user inputs\n",
        "print(\"=\"*80)\n",
        "print(\"🤖 AGENTIC WEB CRAWLER - CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# URL Input\n",
        "start_url = input(\"🌐 Enter the website URL to crawl: \").strip()\n",
        "if not start_url.startswith(('http://', 'https://')):\n",
        "    start_url = 'https://' + start_url\n",
        "\n",
        "print()\n",
        "print(\"📝 What information are you looking for?\")\n",
        "print(\"   Examples:\")\n",
        "print(\"   - 'Product names, prices, and availability'\")\n",
        "print(\"   - 'Blog articles with titles and publication dates'\")\n",
        "print(\"   - 'Documentation pages with code examples'\")\n",
        "print(\"   - 'Contact information and team member details'\")\n",
        "print()\n",
        "\n",
        "# Objective Input\n",
        "crawl_objective = input(\"Your objective: \").strip()\n",
        "if not crawl_objective:\n",
        "    crawl_objective = \"Extract all relevant content and structured data\"\n",
        "\n",
        "print()\n",
        "\n",
        "# Max pages input\n",
        "max_pages_input = input(\"🔢 Maximum pages to crawl (default: 20): \").strip()\n",
        "max_pages = int(max_pages_input) if max_pages_input.isdigit() else 20\n",
        "\n",
        "print()\n",
        "print(\"✅ Configuration complete!\")\n",
        "print(f\"   URL: {start_url}\")\n",
        "print(f\"   Objective: {crawl_objective}\")\n",
        "print(f\"   Max Pages: {max_pages}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Step 7: Run the Intelligent Crawler\n",
        "\n",
        "This will:\n",
        "1. Analyze your objective with AI\n",
        "2. Perform reconnaissance crawl\n",
        "3. Execute targeted deep crawl\n",
        "4. Extract relevant data schema-lessly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize crawler\n",
        "crawler = ImprovedAgenticWebCrawler(\n",
        "    decision_model=\"deepseek-r1:14b\",\n",
        "    extraction_model=\"deepseek-r1:14b\",\n",
        "    max_pages=max_pages\n",
        ")\n",
        "\n",
        "# Set objective\n",
        "crawler.crawl_objective = crawl_objective\n",
        "\n",
        "# Analyze objective with AI\n",
        "await crawler.analyze_user_objective(crawl_objective)\n",
        "\n",
        "# Start crawling\n",
        "scraped_data = await crawler.crawl_website(start_url)\n",
        "\n",
        "print(f\"\\n✅ Successfully crawled {len(scraped_data)} pages\")\n",
        "print(f\"   High-value pages: {len(crawler.high_value_pages)}\")\n",
        "if crawler.page_relevance_scores:\n",
        "    avg_relevance = sum(crawler.page_relevance_scores.values()) / len(crawler.page_relevance_scores)\n",
        "    print(f\"   Average relevance: {avg_relevance:.1f}/10\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Step 8: Display Scraped Data (Raw JSON)\n",
        "\n",
        "View the raw extracted data in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare output data (only relevant pages)\n",
        "output_data = {\n",
        "    \"objective\": crawl_objective,\n",
        "    \"total_pages_crawled\": len(scraped_data),\n",
        "    \"high_value_pages\": len(crawler.high_value_pages),\n",
        "    \"extracted_data\": [\n",
        "        {\n",
        "            \"url\": page['url'],\n",
        "            \"page_type\": page.get('page_type', 'unknown'),\n",
        "            \"relevance\": page.get('relevance_score', 0),\n",
        "            \"extracted_content\": page.get('ai_extraction', {}).get('key_content', {})\n",
        "        }\n",
        "        for page in scraped_data\n",
        "        if page.get('relevance_score', 0) >= 5  # Only include relevant pages\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Display as formatted JSON\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📦 SCRAPED DATA (JSON FORMAT)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "print(json.dumps(output_data, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 Step 9: AI-Powered Summary Generation\n",
        "\n",
        "Use the LLM to generate a comprehensive summary that directly answers your specific question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate AI summary based on extracted data\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🤖 GENERATING AI-POWERED COMPREHENSIVE ANSWER\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Prepare data for AI analysis - Include moderately relevant and above\n",
        "all_pages = [p for p in scraped_data if p.get('relevance_score', 0) >= 4]  # Balanced threshold\n",
        "\n",
        "if not all_pages:\n",
        "    print(\"⚠ No relevant data found to summarize.\")\n",
        "else:\n",
        "    # Build comprehensive context from ALL extracted data\n",
        "    context_parts = []\n",
        "    for idx, page in enumerate(all_pages[:20], 1):  # Increased from 10 to 20 pages\n",
        "        ai_extraction = page.get('ai_extraction', {})\n",
        "        content = ai_extraction.get('key_content', {})\n",
        "        summary = ai_extraction.get('content_summary', '')\n",
        "        page_type = page.get('page_type', 'unknown')\n",
        "        \n",
        "        context_parts.append(f\"=== SOURCE {idx} ({page_type}) ===\")\n",
        "        context_parts.append(f\"URL: {page['url']}\")\n",
        "        context_parts.append(f\"Summary: {summary}\")\n",
        "        context_parts.append(f\"Extracted Data: {json.dumps(content, indent=2)}\")\n",
        "        context_parts.append(\"=\" * 50)\n",
        "    \n",
        "    context = \"\\n\".join(context_parts)\n",
        "    \n",
        "    # Generate comprehensive summary with AI\n",
        "    summary_prompt = '''You are an expert analyst tasked with answering a user's specific question based on web-scraped data.\n",
        "\n",
        "USER'S QUESTION/OBJECTIVE:\n",
        "\"''' + crawl_objective + '''\"\n",
        "\n",
        "IMPORTANT INSTRUCTIONS:\n",
        "1. Read the user's question VERY CAREFULLY and understand what they are asking for\n",
        "2. Analyze ALL the extracted data below, even if it seems only remotely related\n",
        "3. Your PRIMARY GOAL is to DIRECTLY ANSWER the user's question as comprehensively as possible\n",
        "4. Extract and present EVERY piece of information that could help answer their question\n",
        "5. If the data is incomplete, mention what was found and what might be missing\n",
        "6. Be thorough - include ALL relevant details, facts, numbers, names, descriptions, etc.\n",
        "\n",
        "EXTRACTED DATA FROM ''' + str(len(all_pages)) + ''' WEB PAGES:\n",
        "\n",
        "''' + context[:15000] + '''\n",
        "\n",
        "YOUR TASK:\n",
        "Provide a COMPREHENSIVE, DETAILED answer to the user's question. Structure your response as:\n",
        "\n",
        "1. **DIRECT ANSWER**: Start with a clear, direct answer to their question\n",
        "2. **COMPLETE FINDINGS**: Present ALL relevant information found, organized logically\n",
        "   - Include specific details, numbers, names, descriptions\n",
        "   - Don't summarize too much - be thorough and detailed\n",
        "   - If there are lists or multiple items, present them ALL\n",
        "3. **SUPPORTING DETAILS**: Additional context and related information\n",
        "4. **DATA COMPLETENESS**: Assess if the scraped data fully answers the question\n",
        "5. **CONCLUSION**: Final summary addressing the user's objective\n",
        "\n",
        "CRITICAL: Your answer should be COMPREHENSIVE and DETAILED. The user wants ALL the information available, not just a summary. Include every relevant piece of data you found.'''\n",
        "    \n",
        "    try:\n",
        "        print(\"🤖 Analyzing all scraped data to answer your question...\\n\")\n",
        "        print(f\"📊 Processing {len(all_pages)} pages of extracted content...\")\n",
        "        print(\"⏳ This may take 1-2 minutes for comprehensive analysis...\\n\")\n",
        "        \n",
        "        response = ollama.generate(\n",
        "            model=\"deepseek-r1:14b\",\n",
        "            prompt=summary_prompt\n",
        "        )\n",
        "        \n",
        "        ai_summary = response['response'].strip()\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "        print(\"✨ COMPREHENSIVE ANSWER TO YOUR QUESTION\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "        print(ai_summary)\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error generating AI summary: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 💾 Step 10: Save Results\n",
        "\n",
        "Save the scraped data and analysis to files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save JSON data\n",
        "with open('scraped_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"✅ Saved: scraped_data.json\")\n",
        "\n",
        "# Save AI summary if generated\n",
        "if 'ai_summary' in locals():\n",
        "    with open('ai_answer.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# AI-Generated Answer\\n\\n\")\n",
        "        f.write(f\"**Question/Objective**: {crawl_objective}\\n\\n\")\n",
        "        f.write(f\"**Generated**: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\\n\\n\")\n",
        "        f.write(\"---\\n\\n\")\n",
        "        f.write(ai_summary)\n",
        "    \n",
        "    print(\"✅ Saved: ai_answer.md\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎉 CRAWLING AND ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab - download files\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"📥 Downloading files...\\n\")\n",
        "    files.download('scraped_data.json')\n",
        "    if 'ai_summary' in locals():\n",
        "        files.download('ai_answer.md')\n",
        "    \n",
        "    print(\"\\n✅ Files downloaded successfully!\")\n",
        "except ImportError:\n",
        "    print(\"ℹ️ Not running in Google Colab. Files saved to current directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎓 How This Works & Tips\n",
        "\n",
        "### Crawling Strategy\n",
        "The crawler uses a two-phase approach for comprehensive data collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "---\n",
        "\n",
        "## 🎓 How This Works\n",
        "\n",
        "### Phase 1: Reconnaissance (10% of pages)\n",
        "- Explores diverse areas to understand site structure\n",
        "- AI identifies high-value URL patterns\n",
        "- Learns content types and relevance\n",
        "\n",
        "### Phase 2: Targeted Deep Crawl (90% of pages)\n",
        "- Focuses on high-value areas\n",
        "- AI selects most relevant links\n",
        "- Prioritizes pages matching learned patterns\n",
        "\n",
        "### Intelligent AI-Powered Screening\n",
        "- AI evaluates each page with clear relevance criteria (0-10 scale)\n",
        "- Adapts extraction depth based on relevance score\n",
        "- Balances comprehensiveness with quality (threshold: 4+)\n",
        "\n",
        "### AI-Powered Answer Generation\n",
        "- Analyzes up to 20 pages of extracted content\n",
        "- Directly answers your specific question\n",
        "- Includes ALL relevant details, not just summaries\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Tips for Best Results\n",
        "\n",
        "1. **Be Specific**: Clear objectives yield better results\n",
        "   - Good: \"List all product names with prices and availability\"\n",
        "   - Better: \"Find all laptop models, their specifications, prices, and stock status\"\n",
        "\n",
        "2. **Increase Max Pages**: For comprehensive coverage, use 30-50+ pages\n",
        "\n",
        "3. **Monitor Relevance**: Check the relevance scores during crawling\n",
        "\n",
        "4. **Review Raw Data**: Check Step 8 output to see what was extracted\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Troubleshooting\n",
        "\n",
        "**Not Enough Data Scraped:**\n",
        "- Increase `max_pages` parameter (try 50-100)\n",
        "- Make your objective more specific\n",
        "- Check if the website has the information you're looking for\n",
        "\n",
        "**Ollama Issues:**\n",
        "- Ensure server is running (check cell 5)\n",
        "- Model must be downloaded (deepseek-r1:14b)\n",
        "\n",
        "**Memory Issues:**\n",
        "- Use GPU runtime in Colab\n",
        "- Reduce `max_pages` if needed\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Credits\n",
        "\n",
        "- **Crawl4AI** - Intelligent web crawling\n",
        "- **Ollama** - Local LLM inference  \n",
        "- **DeepSeek R1** - Advanced reasoning model\n",
        "- **BeautifulSoup** - HTML parsing\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🎓 How This Works\n",
        "\n",
        "### Phase 1: Reconnaissance\n",
        "- Crawls a small sample of pages to understand site structure\n",
        "- AI analyzes page types and identifies high-value patterns\n",
        "- Learns which URL patterns contain relevant information\n",
        "\n",
        "### Phase 2: Targeted Deep Crawl\n",
        "- Focuses on high-value pages discovered in reconnaissance\n",
        "- AI makes intelligent navigation decisions\n",
        "- Prioritizes links matching learned patterns\n",
        "\n",
        "### Schema-less Extraction\n",
        "- No predefined extraction patterns\n",
        "- AI determines what's important based on your objective\n",
        "- Adapts extraction strategy to each page's content\n",
        "\n",
        "### AI-Powered Analysis\n",
        "- Converts raw data into human-readable insights\n",
        "- Generates comprehensive answers to your questions\n",
        "- Emphasizes information relevant to your objective\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Troubleshooting\n",
        "\n",
        "**Ollama Connection Issues:**\n",
        "- Ensure Ollama server is running (check cell 5)\n",
        "- Model must be downloaded (deepseek-r1:14b)\n",
        "\n",
        "**Memory Issues:**\n",
        "- Reduce `max_pages` parameter\n",
        "- Use a smaller model (e.g., qwen2.5:7b)\n",
        "\n",
        "**Slow Performance:**\n",
        "- Normal for large models like deepseek-r1:14b\n",
        "- Consider using GPU runtime in Colab\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Credits\n",
        "\n",
        "Built with:\n",
        "- **Crawl4AI** - Intelligent web crawling\n",
        "- **Ollama** - Local LLM inference\n",
        "- **DeepSeek R1** - Advanced reasoning model\n",
        "- **BeautifulSoup** - HTML parsing\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
