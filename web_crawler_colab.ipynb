{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Agentic AI Web Crawler - Google Colab Edition\n",
        "\n",
        "**Schema-less AI-Powered Intelligent Web Crawling**\n",
        "\n",
        "This notebook uses:\n",
        "- **Ollama** with **deepseek-r1:14b** for AI-powered decisions\n",
        "- **Crawl4AI** for intelligent web crawling\n",
        "- **AI Navigation** - LLM figures out which links are relevant\n",
        "- **Schema-less Extraction** - LLM determines what's important to scrape\n",
        "- **Two-phase crawling** - Reconnaissance + Targeted Deep Dive\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Install Dependencies\n",
        "\n",
        "Install all required packages for web crawling and AI processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q crawl4ai>=0.3.0 ollama>=0.1.0 aiohttp>=3.9.0 beautifulsoup4>=4.12.0 lxml>=4.9.0\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n",
        "\n",
        "# Install Playwright browsers (required by Crawl4AI)\n",
        "print(\"\\nüì• Installing Playwright browsers...\")\n",
        "!playwright install chromium\n",
        "!playwright install-deps chromium\n",
        "\n",
        "print(\"‚úÖ Playwright browsers installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 2: Setup Ollama Connection\n",
        "\n",
        "**Important**: This notebook expects Ollama to be running locally or accessible via network.\n",
        "\n",
        "For Google Colab, you have two options:\n",
        "1. **Tunnel from local machine**: Run Ollama locally and use ngrok/cloudflare tunnel\n",
        "2. **Install Ollama in Colab**: Install and run Ollama directly in this notebook\n",
        "\n",
        "We'll use option 2 for simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama in Colab\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"‚úÖ Ollama installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start Ollama server in background\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Ollama server\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(5)  # Wait for server to start\n",
        "\n",
        "print(\"‚úÖ Ollama server started!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull the deepseek-r1:14b model\n",
        "print(\"üì• Downloading deepseek-r1:14b model (this may take several minutes)...\")\n",
        "!ollama pull deepseek-r1:14b\n",
        "\n",
        "print(\"‚úÖ Model downloaded and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Step 3: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from collections import defaultdict\n",
        "import ollama\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.extraction_strategy import NoExtractionStrategy\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üï∑Ô∏è Step 4: Define the Agentic Web Crawler Class\n",
        "\n",
        "This class implements:\n",
        "- AI-powered objective analysis\n",
        "- Schema-less content extraction\n",
        "- Intelligent link navigation\n",
        "- Two-phase crawling strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImprovedAgenticWebCrawler:\n",
        "    \"\"\"An intelligent web crawler that uses AI to make navigation decisions and extract content schema-lessly.\"\"\"\n",
        "    \n",
        "    def __init__(self, decision_model: str = \"deepseek-r1:14b\", extraction_model: str = \"deepseek-r1:14b\", max_pages: int = 50):\n",
        "        self.decision_model = decision_model\n",
        "        self.extraction_model = extraction_model\n",
        "        self.max_pages = max_pages\n",
        "        self.visited_urls = set()\n",
        "        self.scraped_data = []\n",
        "        self.base_domain = None\n",
        "        self.crawl_objective = \"\"\n",
        "        self.crawl_objective_analysis = {}\n",
        "        self.desired_data_types = []\n",
        "        self.site_understanding = {\"site_type\": None, \"main_sections\": [], \"content_patterns\": defaultdict(list), \"high_value_url_patterns\": [], \"recommended_focus\": \"\"}\n",
        "        self.page_relevance_scores = {}\n",
        "        self.high_value_pages = []\n",
        "        self.current_phase = \"initialization\"\n",
        "        \n",
        "    async def analyze_user_objective(self, objective: str) -> Dict[str, Any]:\n",
        "        print(\"\\nü§ñ Analyzing your objective with AI...\")\n",
        "        prompt = f\"\"\"You are helping to plan a web crawling operation. Analyze the user's objective and provide a detailed crawl strategy.\n",
        "\n",
        "USER'S OBJECTIVE: \"{objective}\"\n",
        "\n",
        "Analyze this objective and provide:\n",
        "1. What TYPE of data they're looking for (e.g., products, articles, contact info, documentation, etc.)\n",
        "2. What specific FIELDS or attributes they likely want extracted\n",
        "3. What sections of a website would be most valuable\n",
        "4. What URL patterns to prioritize\n",
        "5. What URL patterns to avoid\n",
        "\n",
        "Respond in JSON format:\n",
        "{{\n",
        "  \"data_types\": [\"primary type\", \"secondary type\"],\n",
        "  \"key_fields\": [\"field1\", \"field2\", \"field3\"],\n",
        "  \"valuable_sections\": [\"section1\", \"section2\"],\n",
        "  \"url_patterns_to_seek\": [\"pattern1\", \"pattern2\"],\n",
        "  \"url_patterns_to_avoid\": [\"pattern1\", \"pattern2\"],\n",
        "  \"extraction_strategy\": \"Description of how to approach extraction\",\n",
        "  \"success_criteria\": \"How to know when we have enough data\"\n",
        "}}\n",
        "\n",
        "Be specific and actionable.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = ollama.generate(model=self.decision_model, prompt=prompt)\n",
        "            response_text = response['response'].strip()\n",
        "            if '```json' in response_text:\n",
        "                json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            elif '```' in response_text:\n",
        "                json_match = re.search(r'```\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            analysis = json.loads(response_text)\n",
        "            print(\"\\n‚úì Objective Analysis Complete:\")\n",
        "            print(f\"  ‚Ä¢ Data Types: {', '.join(analysis.get('data_types', []))}\")\n",
        "            print(f\"  ‚Ä¢ Key Fields: {', '.join(analysis.get('key_fields', []))}\")\n",
        "            print(f\"  ‚Ä¢ Focus Areas: {', '.join(analysis.get('valuable_sections', []))}\")\n",
        "            self.crawl_objective_analysis = analysis\n",
        "            self.desired_data_types = analysis.get('data_types', [])\n",
        "            return analysis\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† Analysis error: {e}\")\n",
        "            return {\"data_types\": [\"general content\"], \"key_fields\": [\"title\", \"content\", \"links\"], \"valuable_sections\": [\"main content\"], \"url_patterns_to_seek\": [], \"url_patterns_to_avoid\": [\"login\", \"signup\", \"cart\"], \"extraction_strategy\": \"Extract all available content\", \"success_criteria\": \"Crawl specified number of pages\"}\n",
        "        \n",
        "    def _is_same_domain(self, url: str) -> bool:\n",
        "        if not self.base_domain:\n",
        "            return False\n",
        "        parsed = urlparse(url)\n",
        "        return parsed.netloc == self.base_domain\n",
        "    \n",
        "    def _extract_url_pattern(self, url: str) -> str:\n",
        "        parsed = urlparse(url)\n",
        "        path_parts = [p for p in parsed.path.split('/') if p]\n",
        "        pattern_parts = []\n",
        "        for part in path_parts:\n",
        "            if part.isdigit() or len(part) > 30:\n",
        "                pattern_parts.append('*')\n",
        "            else:\n",
        "                pattern_parts.append(part)\n",
        "        return '/' + '/'.join(pattern_parts) if pattern_parts else '/'\n",
        "    \n",
        "    def _find_similar_visited_urls(self, url: str, limit: int = 3) -> List[str]:\n",
        "        pattern = self._extract_url_pattern(url)\n",
        "        similar = []\n",
        "        for visited in self.visited_urls:\n",
        "            if self._extract_url_pattern(visited) == pattern:\n",
        "                similar.append(visited)\n",
        "                if len(similar) >= limit:\n",
        "                    break\n",
        "        return similar\n",
        "    \n",
        "    async def _extract_content_with_ai(self, html: str, url: str, markdown: str = \"\") -> Dict[str, Any]:\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        for tag in soup.find_all(['nav', 'header', 'footer', 'script', 'style']):\n",
        "            tag.decompose()\n",
        "        page_text = soup.get_text(separator='\\n', strip=True)\n",
        "        content_to_analyze = markdown[:4000] if markdown else page_text[:4000]\n",
        "        headers = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2', 'h3']) if h.get_text(strip=True)][:15]\n",
        "        main_links = []\n",
        "        for a in soup.find_all('a', href=True)[:30]:\n",
        "            text = a.get_text(strip=True)\n",
        "            if text and len(text) > 2:\n",
        "                main_links.append(text)\n",
        "        \n",
        "        prompt = f\"\"\"You are analyzing a web page to extract relevant information based on a specific objective.\n",
        "\n",
        "CRAWL OBJECTIVE: {self.crawl_objective}\n",
        "\n",
        "TARGET DATA TYPES: {', '.join(self.desired_data_types)}\n",
        "KEY FIELDS TO EXTRACT: {', '.join(self.crawl_objective_analysis.get('key_fields', []))}\n",
        "\n",
        "PAGE URL: {url}\n",
        "\n",
        "PAGE HEADERS:\n",
        "{chr(10).join(headers[:10])}\n",
        "\n",
        "PAGE CONTENT (excerpt):\n",
        "{content_to_analyze}\n",
        "\n",
        "MAIN LINKS:\n",
        "{', '.join(main_links[:15])}\n",
        "\n",
        "YOUR TASK:\n",
        "1. Determine the TYPE of page this is (e.g., \"product listing\", \"article\", \"documentation\", \"about page\", \"homepage\", etc.)\n",
        "2. Rate how RELEVANT this page is to the crawl objective (0-10 scale)\n",
        "3. Extract ALL relevant structured data that matches the objective\n",
        "4. Be FLEXIBLE - adapt your extraction schema to what's actually on the page\n",
        "\n",
        "Respond in JSON format:\n",
        "{{\n",
        "  \"page_type\": \"...\",\n",
        "  \"relevance_score\": 0-10,\n",
        "  \"key_content\": {{\n",
        "    // Extract whatever structured data is relevant\n",
        "    // Examples: \"items\": [...], \"article_text\": \"...\", \"metadata\": {{...}}\n",
        "    // Adapt to the page content and objective\n",
        "  }},\n",
        "  \"reasoning\": \"Brief explanation of why this page is/isn't relevant\",\n",
        "  \"content_summary\": \"One sentence summary of page content\"\n",
        "}}\n",
        "\n",
        "Be thorough but concise. Extract actual data, not descriptions.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = ollama.generate(model=self.extraction_model, prompt=prompt)\n",
        "            response_text = response['response'].strip()\n",
        "            if '```json' in response_text:\n",
        "                json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            elif '```' in response_text:\n",
        "                json_match = re.search(r'```\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            extracted = json.loads(response_text)\n",
        "            self._update_site_knowledge(url, extracted)\n",
        "            return extracted\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† AI extraction error: {str(e)[:100]}\")\n",
        "            return {\"page_type\": \"unknown\", \"relevance_score\": 5, \"key_content\": {\"title\": soup.find('title').get_text() if soup.find('title') else \"No title\", \"headers\": headers, \"text_excerpt\": page_text[:500]}, \"reasoning\": \"Fallback extraction due to error\", \"content_summary\": \"Content extracted with fallback method\"}\n",
        "    \n",
        "    def _update_site_knowledge(self, url: str, extraction_result: Dict):\n",
        "        relevance = extraction_result.get('relevance_score', 0)\n",
        "        self.page_relevance_scores[url] = relevance\n",
        "        if relevance >= 7:\n",
        "            self.high_value_pages.append(url)\n",
        "            url_pattern = self._extract_url_pattern(url)\n",
        "            if url_pattern not in self.site_understanding['high_value_url_patterns']:\n",
        "                self.site_understanding['high_value_url_patterns'].append(url_pattern)\n",
        "            page_type = extraction_result.get('page_type')\n",
        "            if page_type:\n",
        "                self.site_understanding['content_patterns'][page_type].append({'url': url, 'pattern': url_pattern, 'relevance': relevance})\n",
        "    \n",
        "    def _normalize_url(self, url: str, base_url: str) -> str:\n",
        "        url = urljoin(base_url, url)\n",
        "        parsed = urlparse(url)\n",
        "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
        "    \n",
        "    async def _extract_links_with_context(self, html: str, base_url: str) -> List[Dict]:\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        links_with_context = []\n",
        "        avoid_patterns = self.crawl_objective_analysis.get('url_patterns_to_avoid', [])\n",
        "        avoid_patterns.extend(['javascript:', 'mailto:', 'tel:', '#', '.jpg', '.png', '.pdf', '.css', '.js'])\n",
        "        \n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link.get('href')\n",
        "            if any(pattern.lower() in href.lower() for pattern in avoid_patterns):\n",
        "                continue\n",
        "            try:\n",
        "                full_url = self._normalize_url(href, base_url)\n",
        "            except:\n",
        "                continue\n",
        "            if not self._is_same_domain(full_url) or full_url in self.visited_urls:\n",
        "                continue\n",
        "            anchor_text = link.get_text(strip=True)\n",
        "            if not anchor_text or len(anchor_text) < 2:\n",
        "                continue\n",
        "            parent = link.parent\n",
        "            context_text = parent.get_text(strip=True)[:200] if parent else \"\"\n",
        "            title = link.get('title', '')\n",
        "            aria_label = link.get('aria-label', '')\n",
        "            is_in_nav = bool(link.find_parent(['nav', 'header']))\n",
        "            is_in_main = bool(link.find_parent('main'))\n",
        "            is_prominent = bool(link.find_parent(['h1', 'h2', 'h3']))\n",
        "            \n",
        "            links_with_context.append({'url': full_url, 'anchor_text': anchor_text, 'context': context_text, 'title': title, 'aria_label': aria_label, 'is_navigation': is_in_nav, 'is_main_content': is_in_main, 'is_prominent': is_prominent, 'url_path': urlparse(full_url).path})\n",
        "        return links_with_context\n",
        "    \n",
        "    async def _score_url_relevance(self, link_info: Dict) -> Dict:\n",
        "        url = link_info['url']\n",
        "        similar_urls = self._find_similar_visited_urls(url)\n",
        "        historical_scores = [self.page_relevance_scores.get(u, 0) for u in similar_urls]\n",
        "        url_pattern = self._extract_url_pattern(url)\n",
        "        pattern_bonus = 2 if url_pattern in self.site_understanding['high_value_url_patterns'] else 0\n",
        "        heuristic_score = 5 + pattern_bonus\n",
        "        if link_info.get('is_main_content'):\n",
        "            heuristic_score += 1\n",
        "        if link_info.get('is_prominent'):\n",
        "            heuristic_score += 1\n",
        "        seek_patterns = self.crawl_objective_analysis.get('url_patterns_to_seek', [])\n",
        "        if any(pattern.lower() in url.lower() for pattern in seek_patterns):\n",
        "            heuristic_score += 2\n",
        "        return {'url': url, 'relevance_score': heuristic_score, 'historical_avg': sum(historical_scores) / len(historical_scores) if historical_scores else 5, 'should_crawl': heuristic_score >= 5, 'priority': 'high' if heuristic_score >= 8 else 'medium' if heuristic_score >= 6 else 'low'}\n",
        "    \n",
        "    async def _ask_ollama_for_navigation_advanced(self, current_url: str, available_links: List[Dict], page_extraction: Dict) -> List[str]:\n",
        "        if not available_links:\n",
        "            return []\n",
        "        scored_links = []\n",
        "        for link_info in available_links[:30]:\n",
        "            score_info = await self._score_url_relevance(link_info)\n",
        "            scored_links.append({**link_info, **score_info})\n",
        "        scored_links.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "        top_candidates = scored_links[:12]\n",
        "        links_summary = []\n",
        "        for i, link in enumerate(top_candidates, 1):\n",
        "            links_summary.append(f\"{i}. [{link['relevance_score']:.1f}] {link['anchor_text'][:50]} ‚Üí {link['url_path']}\")\n",
        "        \n",
        "        prompt = f\"\"\"You are guiding a web crawler. Review these pre-scored URLs and select the best ones to crawl next.\n",
        "\n",
        "CRAWL OBJECTIVE: {self.crawl_objective}\n",
        "\n",
        "PROGRESS:\n",
        "- Pages crawled: {len(self.visited_urls)}/{self.max_pages}\n",
        "- High-value pages: {len(self.high_value_pages)}\n",
        "- Current phase: {self.current_phase}\n",
        "\n",
        "CURRENT PAGE: {current_url}\n",
        "Page type: {page_extraction.get('page_type', 'unknown')}\n",
        "Relevance: {page_extraction.get('relevance_score', '?')}/10\n",
        "Summary: {page_extraction.get('content_summary', 'N/A')}\n",
        "\n",
        "LEARNED PATTERNS:\n",
        "High-value URL patterns: {self.site_understanding['high_value_url_patterns'][:5]}\n",
        "\n",
        "TOP CANDIDATE URLS (with pre-scored relevance [score]):\n",
        "{chr(10).join(links_summary)}\n",
        "\n",
        "Select 3-5 URLs that best match the objective. Consider:\n",
        "1. URLs matching learned high-value patterns\n",
        "2. URLs with high relevance scores\n",
        "3. URLs that explore new areas vs going deeper\n",
        "4. Current progress toward objective\n",
        "\n",
        "Respond with ONLY the numbers (comma-separated, e.g., \"1,3,5,8\").\n",
        "If no links are worth crawling, respond \"NONE\".\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = ollama.generate(model=self.decision_model, prompt=prompt)\n",
        "            answer = response['response'].strip()\n",
        "            if 'NONE' in answer.upper():\n",
        "                print(\"  ‚Üí AI: No valuable links found\")\n",
        "                return []\n",
        "            numbers = re.findall(r'\\d+', answer)\n",
        "            selected_urls = []\n",
        "            for num in numbers[:5]:\n",
        "                idx = int(num) - 1\n",
        "                if 0 <= idx < len(top_candidates):\n",
        "                    selected_urls.append(top_candidates[idx]['url'])\n",
        "            return selected_urls\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† Navigation AI error: {str(e)[:100]}\")\n",
        "            return [link['url'] for link in scored_links[:3]]\n",
        "    \n",
        "    async def _crawl_page(self, url: str, crawler: AsyncWebCrawler) -> Optional[Dict[str, Any]]:\n",
        "        try:\n",
        "            print(f\"üìÑ Crawling: {url}\")\n",
        "            result = await crawler.arun(url=url, extraction_strategy=NoExtractionStrategy(), bypass_cache=True)\n",
        "            if not result.success:\n",
        "                print(f\"  ‚úó Failed to crawl\")\n",
        "                return None\n",
        "            ai_extraction = await self._extract_content_with_ai(result.html, url, result.markdown if result.markdown else \"\")\n",
        "            page_data = {\"url\": url, \"title\": result.metadata.get(\"title\", \"No title\"), \"description\": result.metadata.get(\"description\", \"\"), \"timestamp\": datetime.now().isoformat(), \"metadata\": result.metadata, \"ai_extraction\": ai_extraction, \"relevance_score\": ai_extraction.get('relevance_score', 0), \"page_type\": ai_extraction.get('page_type', 'unknown')}\n",
        "            relevance = ai_extraction.get('relevance_score', 0)\n",
        "            page_type = ai_extraction.get('page_type', 'unknown')\n",
        "            print(f\"  ‚úì Type: {page_type} | Relevance: {relevance}/10\")\n",
        "            key_content = ai_extraction.get('key_content', {})\n",
        "            if key_content:\n",
        "                content_types = list(key_content.keys())\n",
        "                print(f\"  ‚Üí Extracted: {', '.join(content_types[:3])}\")\n",
        "            return page_data\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚úó Error: {str(e)[:80]}\")\n",
        "            return None\n",
        "    \n",
        "    async def _analyze_site_structure(self) -> Dict:\n",
        "        print(\"\\nüîç Analyzing site structure...\")\n",
        "        page_types = {}\n",
        "        for page in self.scraped_data:\n",
        "            page_type = page.get('page_type', 'unknown')\n",
        "            relevance = page.get('relevance_score', 0)\n",
        "            if page_type not in page_types:\n",
        "                page_types[page_type] = []\n",
        "            page_types[page_type].append(relevance)\n",
        "        pages_summary = []\n",
        "        for page in self.scraped_data[:10]:\n",
        "            pages_summary.append(f\"- {page['url']}: {page['page_type']} (relevance: {page.get('relevance_score', 0)}/10)\")\n",
        "        \n",
        "        prompt = f\"\"\"Analyze the reconnaissance crawl results and provide strategic guidance.\n",
        "\n",
        "CRAWL OBJECTIVE: {self.crawl_objective}\n",
        "\n",
        "PAGES CRAWLED IN RECONNAISSANCE ({len(self.scraped_data)} pages):\n",
        "{chr(10).join(pages_summary)}\n",
        "\n",
        "PAGE TYPE DISTRIBUTION:\n",
        "{json.dumps({pt: {'count': len(scores), 'avg_relevance': sum(scores)/len(scores)} for pt, scores in page_types.items()}, indent=2)}\n",
        "\n",
        "HIGH-VALUE URL PATTERNS DISCOVERED:\n",
        "{self.site_understanding['high_value_url_patterns']}\n",
        "\n",
        "Provide strategic analysis:\n",
        "1. What TYPE of website is this?\n",
        "2. Which sections/page types are most valuable for the objective?\n",
        "3. What URL patterns should we prioritize in deep crawl?\n",
        "4. What's the recommended crawl strategy going forward?\n",
        "\n",
        "Respond in JSON:\n",
        "{{\n",
        "  \"site_type\": \"...\",\n",
        "  \"most_valuable_page_types\": [\"type1\", \"type2\"],\n",
        "  \"recommended_focus\": \"Description of where to focus\",\n",
        "  \"high_priority_patterns\": [\"pattern1\", \"pattern2\"],\n",
        "  \"strategy\": \"continue_deep | adjust_objective | site_not_suitable\"\n",
        "}}\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = ollama.generate(model=self.decision_model, prompt=prompt)\n",
        "            response_text = response['response'].strip()\n",
        "            if '```json' in response_text:\n",
        "                json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    response_text = json_match.group(1)\n",
        "            analysis = json.loads(response_text)\n",
        "            print(f\"  ‚úì Site Type: {analysis.get('site_type', 'Unknown')}\")\n",
        "            print(f\"  ‚úì Focus: {analysis.get('recommended_focus', 'General crawl')}\")\n",
        "            return analysis\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† Analysis error: {e}\")\n",
        "            return {\"site_type\": \"unknown\", \"most_valuable_page_types\": list(page_types.keys())[:2], \"recommended_focus\": \"Continue crawling all page types\", \"high_priority_patterns\": self.site_understanding['high_value_url_patterns'], \"strategy\": \"continue_deep\"}\n",
        "    \n",
        "    async def crawl_website(self, start_url: str) -> List[Dict[str, Any]]:\n",
        "        parsed_url = urlparse(start_url)\n",
        "        self.base_domain = parsed_url.netloc\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"üöÄ STARTING INTELLIGENT WEB CRAWL\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Target: {start_url}\")\n",
        "        print(f\"Objective: {self.crawl_objective}\")\n",
        "        print(f\"Max pages: {self.max_pages}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        recon_budget = max(5, self.max_pages // 10)\n",
        "        self.current_phase = \"reconnaissance\"\n",
        "        print(f\"üì° PHASE 1: RECONNAISSANCE ({recon_budget} pages)\")\n",
        "        print(f\"{'‚îÄ'*80}\")\n",
        "        \n",
        "        async with AsyncWebCrawler(verbose=False) as crawler:\n",
        "            url_queue = [start_url]\n",
        "            while url_queue and len(self.visited_urls) < recon_budget:\n",
        "                current_url = url_queue.pop(0)\n",
        "                if current_url in self.visited_urls:\n",
        "                    continue\n",
        "                self.visited_urls.add(current_url)\n",
        "                page_data = await self._crawl_page(current_url, crawler)\n",
        "                if page_data:\n",
        "                    self.scraped_data.append(page_data)\n",
        "                    result = await crawler.arun(url=current_url, bypass_cache=True)\n",
        "                    if result.success:\n",
        "                        links = await self._extract_links_with_context(result.html, current_url)\n",
        "                        if links:\n",
        "                            selected = links[:8]\n",
        "                            for link in selected:\n",
        "                                if link['url'] not in self.visited_urls and link['url'] not in url_queue:\n",
        "                                    url_queue.append(link['url'])\n",
        "                print(f\"  Progress: {len(self.visited_urls)}/{recon_budget} recon pages\\n\")\n",
        "        \n",
        "        site_analysis = await self._analyze_site_structure()\n",
        "        self.site_understanding.update(site_analysis)\n",
        "        \n",
        "        deep_budget = self.max_pages - len(self.visited_urls)\n",
        "        self.current_phase = \"deep_crawl\"\n",
        "        print(f\"\\n{'‚îÄ'*80}\")\n",
        "        print(f\"üéØ PHASE 2: TARGETED DEEP CRAWL ({deep_budget} pages)\")\n",
        "        print(f\"{'‚îÄ'*80}\\n\")\n",
        "        \n",
        "        async with AsyncWebCrawler(verbose=False) as crawler:\n",
        "            url_queue = []\n",
        "            for page in self.scraped_data:\n",
        "                if page.get('relevance_score', 0) >= 6:\n",
        "                    result = await crawler.arun(url=page['url'], bypass_cache=True)\n",
        "                    if result.success:\n",
        "                        links = await self._extract_links_with_context(result.html, page['url'])\n",
        "                        for link in links[:5]:\n",
        "                            if link['url'] not in self.visited_urls:\n",
        "                                url_queue.append(link['url'])\n",
        "            url_queue = list(dict.fromkeys(url_queue))\n",
        "            \n",
        "            while url_queue and len(self.visited_urls) < self.max_pages:\n",
        "                current_url = url_queue.pop(0)\n",
        "                if current_url in self.visited_urls:\n",
        "                    continue\n",
        "                self.visited_urls.add(current_url)\n",
        "                page_data = await self._crawl_page(current_url, crawler)\n",
        "                if page_data:\n",
        "                    self.scraped_data.append(page_data)\n",
        "                    result = await crawler.arun(url=current_url, bypass_cache=True)\n",
        "                    if result.success:\n",
        "                        links = await self._extract_links_with_context(result.html, current_url)\n",
        "                        if links:\n",
        "                            selected_urls = await self._ask_ollama_for_navigation_advanced(current_url, links, page_data.get('ai_extraction', {}))\n",
        "                            print(f\"  ‚Üí AI selected {len(selected_urls)} links\")\n",
        "                            for url in selected_urls:\n",
        "                                if url not in self.visited_urls and url not in url_queue:\n",
        "                                    url_queue.append(url)\n",
        "                print(f\"  Progress: {len(self.visited_urls)}/{self.max_pages} | Queue: {len(url_queue)} | High-value: {len(self.high_value_pages)}\\n\")\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"‚úÖ CRAWL COMPLETE\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Total pages: {len(self.scraped_data)}\")\n",
        "        print(f\"High-value pages: {len(self.high_value_pages)}\")\n",
        "        print(f\"Avg relevance: {sum(self.page_relevance_scores.values())/len(self.page_relevance_scores) if self.page_relevance_scores else 0:.1f}/10\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        return self.scraped_data\n",
        "\n",
        "print(\"‚úÖ ImprovedAgenticWebCrawler class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_content_recursive(content: Any, level: int = 3) -> list:\n",
        "    \"\"\"Recursively format extracted content into readable markdown.\"\"\"\n",
        "    lines = []\n",
        "    if isinstance(content, dict):\n",
        "        for key, value in content.items():\n",
        "            title = key.replace('_', ' ').title()\n",
        "            if isinstance(value, (dict, list)):\n",
        "                lines.append(f\"{'#' * level} {title}\\n\")\n",
        "                lines.extend(format_content_recursive(value, level + 1))\n",
        "            else:\n",
        "                lines.append(f\"**{title}**: {value}\\n\")\n",
        "    elif isinstance(content, list):\n",
        "        if not content:\n",
        "            lines.append(\"*No items found*\\n\")\n",
        "        elif all(isinstance(item, dict) for item in content):\n",
        "            for idx, item in enumerate(content, 1):\n",
        "                lines.append(f\"\\n### Item {idx}\\n\")\n",
        "                lines.extend(format_content_recursive(item, level + 1))\n",
        "        else:\n",
        "            for item in content:\n",
        "                if isinstance(item, str):\n",
        "                    lines.append(f\"- {item}\\n\")\n",
        "                else:\n",
        "                    lines.append(f\"- {str(item)}\\n\")\n",
        "    else:\n",
        "        lines.append(f\"{content}\\n\")\n",
        "    return lines\n",
        "\n",
        "def generate_human_readable_report(scraped_data: List[Dict], objective: str) -> str:\n",
        "    \"\"\"Generate a human-readable report from scraped data.\"\"\"\n",
        "    md = []\n",
        "    md.append(\"# üìä Extracted Data Analysis Report\\n\")\n",
        "    md.append(f\"**Generated**: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\\n\")\n",
        "    md.append(f\"**Search Objective**: {objective}\\n\")\n",
        "    md.append(\"---\\n\")\n",
        "    relevant_pages = [p for p in scraped_data if p.get('relevance_score', 0) >= 5]\n",
        "    if not relevant_pages:\n",
        "        md.append(\"## No Relevant Data Found\\n\")\n",
        "        md.append(\"No pages matched the objective with sufficient relevance.\\n\")\n",
        "    else:\n",
        "        md.append(f\"## Summary\\n\")\n",
        "        md.append(f\"Found **{len(relevant_pages)}** pages with relevant information.\\n\\n\")\n",
        "        md.append(\"---\\n\")\n",
        "        for idx, page in enumerate(relevant_pages, 1):\n",
        "            url = page.get('url', 'Unknown URL')\n",
        "            ai_extraction = page.get('ai_extraction', {})\n",
        "            content = ai_extraction.get('key_content', {})\n",
        "            relevance = page.get('relevance_score', 0)\n",
        "            page_type = page.get('page_type', 'unknown')\n",
        "            summary = ai_extraction.get('content_summary', 'N/A')\n",
        "            if not content or content == {}:\n",
        "                continue\n",
        "            md.append(f\"\\n## üìÑ Source {idx}\\n\")\n",
        "            md.append(f\"**Page Type**: {page_type}\\n\")\n",
        "            md.append(f\"**Relevance Score**: {relevance}/10\\n\")\n",
        "            md.append(f\"**Summary**: {summary}\\n\\n\")\n",
        "            md.extend(format_content_recursive(content, level=3))\n",
        "            md.append(f\"\\n**Source URL**: [{url}]({url})\\n\")\n",
        "            md.append(\"\\n---\\n\")\n",
        "    return ''.join(md)\n",
        "\n",
        "print(\"‚úÖ Data formatting functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get user inputs\n",
        "print(\"=\"*80)\n",
        "print(\"ü§ñ AGENTIC WEB CRAWLER - CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print()\n",
        "\n",
        "# URL Input\n",
        "start_url = input(\"üåê Enter the website URL to crawl: \").strip()\n",
        "if not start_url.startswith(('http://', 'https://')):\n",
        "    start_url = 'https://' + start_url\n",
        "\n",
        "print()\n",
        "print(\"üìù What information are you looking for?\")\n",
        "print(\"   Examples:\")\n",
        "print(\"   - 'Product names, prices, and availability'\")\n",
        "print(\"   - 'Blog articles with titles and publication dates'\")\n",
        "print(\"   - 'Documentation pages with code examples'\")\n",
        "print(\"   - 'Contact information and team member details'\")\n",
        "print()\n",
        "\n",
        "# Objective Input\n",
        "crawl_objective = input(\"Your objective: \").strip()\n",
        "if not crawl_objective:\n",
        "    crawl_objective = \"Extract all relevant content and structured data\"\n",
        "\n",
        "print()\n",
        "\n",
        "# Max pages input\n",
        "max_pages_input = input(\"üî¢ Maximum pages to crawl (default: 20): \").strip()\n",
        "max_pages = int(max_pages_input) if max_pages_input.isdigit() else 20\n",
        "\n",
        "print()\n",
        "print(\"‚úÖ Configuration complete!\")\n",
        "print(f\"   URL: {start_url}\")\n",
        "print(f\"   Objective: {crawl_objective}\")\n",
        "print(f\"   Max Pages: {max_pages}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 7: Run the Intelligent Crawler\n",
        "\n",
        "This will:\n",
        "1. Analyze your objective with AI\n",
        "2. Perform reconnaissance crawl\n",
        "3. Execute targeted deep crawl\n",
        "4. Extract relevant data schema-lessly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize crawler\n",
        "crawler = ImprovedAgenticWebCrawler(\n",
        "    decision_model=\"deepseek-r1:14b\",\n",
        "    extraction_model=\"deepseek-r1:14b\",\n",
        "    max_pages=max_pages\n",
        ")\n",
        "\n",
        "# Set objective\n",
        "crawler.crawl_objective = crawl_objective\n",
        "\n",
        "# Analyze objective with AI\n",
        "await crawler.analyze_user_objective(crawl_objective)\n",
        "\n",
        "# Start crawling\n",
        "scraped_data = await crawler.crawl_website(start_url)\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully crawled {len(scraped_data)} pages\")\n",
        "print(f\"   High-value pages: {len(crawler.high_value_pages)}\")\n",
        "if crawler.page_relevance_scores:\n",
        "    avg_relevance = sum(crawler.page_relevance_scores.values()) / len(crawler.page_relevance_scores)\n",
        "    print(f\"   Average relevance: {avg_relevance:.1f}/10\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 8: Display Scraped Data (Raw JSON)\n",
        "\n",
        "View the raw extracted data in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare output data (only relevant pages)\n",
        "output_data = {\n",
        "    \"objective\": crawl_objective,\n",
        "    \"total_pages_crawled\": len(scraped_data),\n",
        "    \"high_value_pages\": len(crawler.high_value_pages),\n",
        "    \"extracted_data\": [\n",
        "        {\n",
        "            \"url\": page['url'],\n",
        "            \"page_type\": page.get('page_type', 'unknown'),\n",
        "            \"relevance\": page.get('relevance_score', 0),\n",
        "            \"extracted_content\": page.get('ai_extraction', {}).get('key_content', {})\n",
        "        }\n",
        "        for page in scraped_data\n",
        "        if page.get('relevance_score', 0) >= 5  # Only include relevant pages\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Display as formatted JSON\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üì¶ SCRAPED DATA (JSON FORMAT)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "print(json.dumps(output_data, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Step 9: Generate Human-Readable Analysis\n",
        "\n",
        "Convert the scraped data into a comprehensive, human-readable report that answers your objective.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate human-readable report\n",
        "report = generate_human_readable_report(scraped_data, crawl_objective)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä HUMAN-READABLE ANALYSIS REPORT\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Step 10: AI-Powered Summary Generation\n",
        "\n",
        "Use the LLM to generate a comprehensive summary answering your specific question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate AI summary based on extracted data\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ü§ñ GENERATING AI-POWERED SUMMARY\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Prepare data for AI analysis\n",
        "relevant_pages = [p for p in scraped_data if p.get('relevance_score', 0) >= 5]\n",
        "\n",
        "if not relevant_pages:\n",
        "    print(\"‚ö† No relevant data found to summarize.\")\n",
        "else:\n",
        "    # Build context from extracted data\n",
        "    context_parts = []\n",
        "    for idx, page in enumerate(relevant_pages[:10], 1):\n",
        "        ai_extraction = page.get('ai_extraction', {})\n",
        "        content = ai_extraction.get('key_content', {})\n",
        "        summary = ai_extraction.get('content_summary', '')\n",
        "        \n",
        "        context_parts.append(f\"Source {idx} ({page['url']}):\")\n",
        "        context_parts.append(f\"Summary: {summary}\")\n",
        "        context_parts.append(f\"Data: {json.dumps(content, indent=2)}\")\n",
        "        context_parts.append(\"---\")\n",
        "    \n",
        "    context = \"\\n\".join(context_parts)\n",
        "    \n",
        "    # Generate summary with AI\n",
        "    summary_prompt = '''You are analyzing data extracted from a web crawling operation.\n",
        "\n",
        "USER'S QUESTION/OBJECTIVE:\n",
        "''' + crawl_objective + '''\n",
        "\n",
        "EXTRACTED DATA FROM ''' + str(len(relevant_pages)) + ''' RELEVANT PAGES:\n",
        "''' + context[:8000] + '''\n",
        "\n",
        "YOUR TASK:\n",
        "Based on the extracted data above, provide a comprehensive answer to the user's question/objective.\n",
        "\n",
        "Format your response as:\n",
        "1. **Executive Summary**: A brief overview answering the main question\n",
        "2. **Key Findings**: Bullet points of the most important information discovered\n",
        "3. **Detailed Analysis**: Full sentences and paragraphs elaborating on the findings\n",
        "4. **Data Points**: Specific facts, numbers, or details extracted\n",
        "5. **Conclusion**: Final thoughts and recommendations\n",
        "\n",
        "Be thorough, accurate, and emphasize information most relevant to the user's objective.\n",
        "Use clear headings, bullet points, and well-structured paragraphs.'''\n",
        "    \n",
        "    try:\n",
        "        print(\"ü§ñ Generating comprehensive analysis...\\n\")\n",
        "        response = ollama.generate(\n",
        "            model=\"deepseek-r1:14b\",\n",
        "            prompt=summary_prompt\n",
        "        )\n",
        "        \n",
        "        ai_summary = response['response'].strip()\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "        print(\"‚ú® AI-GENERATED COMPREHENSIVE ANALYSIS\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "        print(ai_summary)\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error generating AI summary: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save JSON data\n",
        "with open('scraped_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"‚úÖ Saved: scraped_data.json\")\n",
        "\n",
        "# Save human-readable report\n",
        "with open('analysis_report.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"‚úÖ Saved: analysis_report.md\")\n",
        "\n",
        "# Save AI summary if generated\n",
        "if 'ai_summary' in locals():\n",
        "    with open('ai_summary.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# AI-Generated Analysis\\n\\n\")\n",
        "        f.write(f\"**Objective**: {crawl_objective}\\n\\n\")\n",
        "        f.write(f\"**Generated**: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\\n\\n\")\n",
        "        f.write(\"---\\n\\n\")\n",
        "        f.write(ai_summary)\n",
        "    \n",
        "    print(\"‚úÖ Saved: ai_summary.md\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ CRAWLING AND ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Step 12: Download Results (Optional)\n",
        "\n",
        "Download the generated files to your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab - download files\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"üì• Downloading files...\\n\")\n",
        "    files.download('scraped_data.json')\n",
        "    files.download('analysis_report.md')\n",
        "    if 'ai_summary' in locals():\n",
        "        files.download('ai_summary.md')\n",
        "    \n",
        "    print(\"\\n‚úÖ Files downloaded successfully!\")\n",
        "except ImportError:\n",
        "    print(\"‚ÑπÔ∏è Not running in Google Colab. Files saved to current directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì How This Works\n",
        "\n",
        "### Phase 1: Reconnaissance\n",
        "- Crawls a small sample of pages to understand site structure\n",
        "- AI analyzes page types and identifies high-value patterns\n",
        "- Learns which URL patterns contain relevant information\n",
        "\n",
        "### Phase 2: Targeted Deep Crawl\n",
        "- Focuses on high-value pages discovered in reconnaissance\n",
        "- AI makes intelligent navigation decisions\n",
        "- Prioritizes links matching learned patterns\n",
        "\n",
        "### Schema-less Extraction\n",
        "- No predefined extraction patterns\n",
        "- AI determines what's important based on your objective\n",
        "- Adapts extraction strategy to each page's content\n",
        "\n",
        "### AI-Powered Analysis\n",
        "- Converts raw data into human-readable insights\n",
        "- Generates comprehensive answers to your questions\n",
        "- Emphasizes information relevant to your objective\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Troubleshooting\n",
        "\n",
        "**Ollama Connection Issues:**\n",
        "- Ensure Ollama server is running (check cell 5)\n",
        "- Model must be downloaded (deepseek-r1:14b)\n",
        "\n",
        "**Memory Issues:**\n",
        "- Reduce `max_pages` parameter\n",
        "- Use a smaller model (e.g., qwen2.5:7b)\n",
        "\n",
        "**Slow Performance:**\n",
        "- Normal for large models like deepseek-r1:14b\n",
        "- Consider using GPU runtime in Colab\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Credits\n",
        "\n",
        "Built with:\n",
        "- **Crawl4AI** - Intelligent web crawling\n",
        "- **Ollama** - Local LLM inference\n",
        "- **DeepSeek R1** - Advanced reasoning model\n",
        "- **BeautifulSoup** - HTML parsing\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
